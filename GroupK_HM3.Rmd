---
title: "GroupK_HM3"
author: "G. Lucarelli"
date: ""
output:
  html_document:
    toc: true
    toc_depth: 3
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS: Chapter 4, exercise 4.24

Refer to the vegetarian survey result in Exercise 4.6, with $n = 25$ and no vegetarians.

(a) Find the Bayesian estimate of $\pi$ using a beta prior distribution with α = β equal (i) 0.5, (ii) 1.0,(iii) 10.0.Explain how the choice of prior distribution affects the posterior mean
estimate.

(b) If you were planning how to take a larger survey from the same population, explain how
you can use the posterior results of the previous survey with $n = 25$ based on the prior
with α = β = 1 to form the prior distribution to use with the new survey results.


### Solution

We know that the prior distribution in a $\text{Beta}(\alpha,\beta)$ and that the likelihood for the binomial distribution is:

$$
L(\pi;y)=\binom{n}{y}\pi^y(1-\pi)^{n-y}=(1-\pi)^{25}\,.
$$
Under this assumptions we know that the posterior distribution is given by a Beta distribution with parameters $\alpha + y$ and $\beta + n - y$. The posterior mean is therefore $\frac{\alpha + y}{\alpha + \beta + n}$.

$$
\text{Posterior}(\pi|y=0)=\text{Beta}(\alpha + y, \beta + n - y)
$$

#### (a) 
For the three cases, we have:

(i) $\alpha = \beta = 0.5$ is the non-informative Jeffreys' prior. The posterior mean is $\frac{0.5}{0.5 + 0.5 + 25} = 0.019$.

```{r}
alpha <- 0.5
beta <- 0.5
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution")

curve(dbeta(x, alpha, beta), from = 0, to = 1, add = TRUE, col = "red")
curve((1-x)^25, from = 0, to = 1, add = TRUE, col = "blue")

legend("topright", legend = c("Posterior", "Prior", "Likelihood"), col = c("black", "red", "blue"), lty = 1)

```

(ii) $\alpha = \beta = 1.0$ it is the uniform distribution in the interval $[0,1]$, is a prior that gives equal weight to the two possible outcomes. The posterior mean is $\frac{1.0}{1.0 + 1.0 + 25} = 0.038$.

```{r}
alpha <- 1.0
beta <- 1.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution")
curve(dbeta(x, alpha, beta), from = 0, to = 1, add = TRUE, col = "red")
curve((1-x)^25, from = 0, to = 1, add = TRUE, col = "blue")

legend("topright", legend = c("Posterior", "Prior", "Likelihood"), col = c("black", "red", "blue"), lty = 1)
```

(iii) $\alpha = \beta = 10.0$ is a prior that gives more weight to the value of $\pi = 0.5$. The posterior mean is $\frac{10.0}{10.0 + 10.0 + 25} = 0.222$.

```{r}
alpha <- 10.0
beta <- 10.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution")
curve(dbeta(x, alpha, beta), from = 0, to = 1, add = TRUE, col = "red")
curve((1-x)^25, from = 0, to = 1, add = TRUE, col = "blue")

legend("topright", legend = c("Posterior", "Prior", "Likelihood"), col = c("black", "red", "blue"), lty = 1)
```

```{r}
# plot of the three posterior distributions
alpha <- 0.5
beta <- 0.5
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution", col = "red")

alpha <- 1.0
beta <- 1.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, add = TRUE, col = "blue")

alpha <- 10.0
beta <- 10.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, add = TRUE, col = "green")

legend("topright", legend = c("Jeffreys' prior", "Uniform prior", "Beta(10,10)"), col = c("red", "blue", "green"), lty = 1)
```

We can see that for more and more informative priors choices, the posterior distribution is shifting towards larger values of $\pi$, even for this case where there are no vegetarians in the sample. Moreover, we can see that the effects of the Jeffreys' prior and the one of the uniform distribution are pretty similiar.

#### (b) 
If we were planning to take a larger survey from the same population, we could use the posterior results of the previous survey as the prior for the new one. For a new survey with $n'$ observations and $y'$ successes, the new posterior distribution, based on the previous posterior distribution $\text{Beta}(1,26)$, is given by:

$$
\text{Posterior}(\pi|y')=\text{Beta}(1 + y', 26 + n' - y')\,.
$$

## FSDS: Chapter 4, exercise 4.62

For the bootstrap method, explain the similarity and difference between the true sampling
distribution of $\hat \theta$ and the empirically-generated bootstrap distribution in terms of its center and its spread.

### Solution

In the frequentistic (classical) approach we assume that If we could repeat the sample many time, from the true distribution of the underlying random variable, we would be able to build the true sampling distribution of an estimator $\hat\theta$.

The idea of the bootstrap is to build many bootstrap samples (i.e. samples with replacement from the original data) and then compute an empirical distribution of the estimator, with the aim of aprroximate its sampling distribution. In particular, the set $\{\hat\theta_b^*\}_{b=1}^B$ gives the empirical (bootstrap) distribution of the estimator $\hat\theta$.

The link with the frequntisic approach is given by the fact that the empirical distribution of the random variable approaches the true one in the limit of the sample size that tends to infinite.

**Key differences:**

Center:

- True sampling distribution centers on the true population parameter $\theta$
$$
\text{E}[\hat\theta]=\theta
$$
- Bootstrap distribution centers on the sample estimate $\hat\theta$, which approximates but may differ from the true value $\theta$.
$$
\text{E}[\hat\theta^*]=\hat\theta
$$

Spread:

- True sampling distribution's spread reflects actual sampling variability in the population:
$$
\text{Var}(\hat\theta) = \text{E}[(\hat\theta-\theta)^2]\\ \theta=\text{E}[\hat\theta]
$$
where the expected value is take over the population distribution;
- Bootstrap distribution's spread approximates this but tends to underestimate or overestimate the true variability since it resamples from a single finite sample rather than the full population.

$$
\widehat{\text{Var}}_{boot}=\frac{1}{B}\sum_{b=1}^B(\hat\theta_b^*-\hat\theta^*)^2
$$
$$
\hat\theta^*=\frac{1}{B}\sum_{b=1}^B\hat\theta_b^*
$$



## FSDS: Chapter 8, exercise 8.4

Refer to Exercise 8.1. Construct a classification tree, and prune strongly until the tree uses
a single explanatory variable. Which crabs were predicted to have satellites? How does the
proportion of correct predictions compare with the more complex tree in Figure 8.2?

### Solution

```{r, include=FALSE}
library(rpart)
library(rpart.plot)
library(caret)
```


```{r}
url <- "https://raw.github.com/stat4DS/data/main/Crabs.dat"
Crabs <- read.table(url, header = TRUE)
str(Crabs)
```


```{r}
# set y as a factor, required by rpart
Crabs$y <- as.factor(Crabs$y)
Crabs$color <- as.factor(Crabs$color)
Crabs$spine <- as.factor(Crabs$spine)

fit <- rpart(y ~ weight + width + color + spine, data = Crabs)
plotcp(fit)
rpart.plot(fit)
```
```{r}
# cp such that nsplit = 1

cp.onesplit<-fit$cptable[, "CP"][which(fit$cptable[, "nsplit"] ==1)]

# prune the tree
p.fit <- prune(fit, cp=cp.onesplit)
rpart.plot(p.fit, extra=1, digits=4, box.palette="auto")
```

We can conclude that crabs with width < 25.85 are predicted to have satellites.

Let's calculate the proportion of correct predictions, without cross-validation (similarly to what we did in the book example):

```{r}
predict <- predict(p.fit, type = "prob" , newdata = Crabs)[,2]>0.5

# confusion matrix
confusion.m <- table(Crabs$y,predict)
confusion.m
```
```{r}
# accuracy
sum(diag(confusion.m))/nrow(Crabs)
```

We can see that the percentage of correct predictions is lower than the proportion of correct predictions in the example (0.751), but not much lower considering the fact that the tree is much less complex.


## LAB: 
Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i\sim\text{Exponential}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

$π(λ)=\text{Beta}(4,2)$;

$π(λ)=\text{Normal}(1,2)$;

$π(λ)=\text{Gamma}(4,2)$;

Now, compute your posterior as $π(λ|y)\propto L(λ;y)π(λ)$ for the selected prior. If your first choice was correct, you will be able to compute it analytically.

### Solution

Since the parameter $\lambda$ is such that $\lambda>0$, we need a prior defined on $\mathbb{R}^+$. The normal is not suitable since is defined over $[-\infty, + \infty]$ and the $\text{Beta}$ too, beeing defined over $[0,1]$. The only suitable choice is the $\text{Gamma}(4,2)$.

$$
\text{Gamma}(4,2)=\frac{2^4\lambda^3e^{-2\lambda}}{\Gamma(4)}=\frac{3}{8}\lambda^3e^{-2\lambda}\\
\pi(\lambda|y)\propto\frac{3}{8}\lambda^3e^{-2\lambda}\prod_{i=1}^n\lambda e^{-\lambda y_i} \propto \lambda^{n+3}e^{-\lambda(2+\sum_i^n y_i)}\\
$$
Remembering that a gamma distribution is
$$
f(\lambda; \alpha, \beta) = \frac{\beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda}}{\Gamma(\alpha)}, \quad \lambda \geq 0
$$
we can see that the posterior distribution is distributed like a new gamma distribution of parameters $n+4$, $2+\sum_i^n y_i$
$$
\pi(\lambda|y) = \text{Gamma}(n+4, 2+\sum_i^n y_i)\, .
$$

## ISLR: chapter 6, exercises 6.9 (points (a)-(d))

In this exercise, we will predict the number of applications received using the other variables in the `College` data set.

(a) Split the data set into a training set and a test set.

(b) Fit a linear model using least squares on the training set, and report the test error obtained.

(c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

(d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

### Solution

#### a: SPLIT THE DATA
```{r}
College <- read.csv("https://www.statlearning.com/s/College.csv", header = TRUE)
# remove the college name
College <- College[, -1]

College$Private <- as.numeric(factor(College$Private))

set.seed(1)
n <- nrow(College)
idx.train <- sample(1:n, n*0.75, replace = FALSE)

College.train <- College[idx.train,]
College.test <- College[-idx.train,]

```

#### b: LINEAR REGRESSION
```{r}
y.test <- College.test$Apps
n.test <- nrow(College.test)

linear.mod <- lm(Apps ~., data = College.train)

predictions <- predict.lm(linear.mod, newdata=College.test)

errors <- y.test - predictions

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("MAE:", round(mae, 2)))
```

#### c: RIDGE REGRESSION
```{r, include=FALSE}
library(glmnet)
```

```{r}
X.train <- as.matrix(College.train[,-2])
X.test <- as.matrix(College.test[,-2])
y.train <- College.train$Apps
y.test <- College.test$Apps

cvfit_ridge <- cv.glmnet(X.train,y.train, alpha = 0)

ridge.pred <- as.numeric(predict(cvfit_ridge, newx = X.test, s = "lambda.min"))

errors <- y.test - ridge.pred

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("MAE:", round(mae, 2)))
```

#### d: LASSO REGRESSION

```{r}
cvfit_lasso <- cv.glmnet(X.train,y.train, alpha = 1)

lasso.pred <- as.numeric(predict(cvfit_lasso, newx = X.test, s = "lambda.min"))

errors <- y.test - lasso.pred

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("MAE:", round(mae, 2)))

# Number of non-zero coefficients in the Lasso model at the optimal lambda
non_zero_coef <- sum(coef(cvfit_lasso, s = "lambda.min") != 0) - 1  # Exclude the intercept
print(paste("Number of non-zero coefficients:", non_zero_coef))
```
```{r}
coef(cvfit_lasso, s = "lambda.min") 
```
Notably, the coefficient for `Terminal`has been shrunk to zero.

## ISLR: chapter 7, exercises 7.9

This question uses the variables `dis` (the weighted mean of distances to five Boston employment centers) and `nox` (nitrogen oxides concentration in parts per 10 million) from the `Boston` data. We will treat `dis` as the predictor and `nox` as the response. 

(a) Use the `poly()` function to fit a cubic polynomial regression to predict `nox` using `dis.` Report the regression output, and plot the resulting data and polynomial fits.

(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.

(d) Use the bs() function to fit a regression spline to predict `nox` using `dis.` Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.


### Solution

## GAM

This question is about using gam for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The mcycle data in the MASS package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use gam to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use lm and poly to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by gam. Use termplot to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use gam to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

### Soltuion
