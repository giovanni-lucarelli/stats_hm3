---
title: "GroupK_HM3"
author: "G. Lucarelli, Z. Jacopo, A. Nicic"
date: ""
output:
  html_document:
    toc: true
    toc_depth: 3
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS: Chapter 4, exercise 4.24

Refer to the vegetarian survey result in Exercise 4.6, with $n = 25$ and no vegetarians.

(a) Find the Bayesian estimate of $\pi$ using a beta prior distribution with α = β equal (i) 0.5, (ii) 1.0,(iii) 10.0.Explain how the choice of prior distribution affects the posterior mean
estimate.

(b) If you were planning how to take a larger survey from the same population, explain how
you can use the posterior results of the previous survey with $n = 25$ based on the prior
with α = β = 1 to form the prior distribution to use with the new survey results.


### Solution

We know that the prior distribution in a $\text{Beta}(\alpha,\beta)$ and that the likelihood for the binomial distribution is:

$$
L(\pi;y)=\binom{n}{y}\pi^y(1-\pi)^{n-y}=(1-\pi)^{25}\,.
$$
Under this assumptions we know that the posterior distribution is given by a Beta distribution with parameters $\alpha + y$ and $\beta + n - y$. The posterior mean is therefore $\frac{\alpha + y}{\alpha + \beta + n}$.

$$
\text{Posterior}(\pi|y=0)=\text{Beta}(\alpha + y, \beta + n - y)
$$

#### (a) 
For the three cases, we have:

(i) $\alpha = \beta = 0.5$ is the non-informative Jeffreys' prior. The posterior mean is $\frac{0.5}{0.5 + 0.5 + 25} = 0.019$.


```{r}
alpha <- 0.5
beta <- 0.5
n <- 25
y <- 0

# Plot posterior
curve(dbeta(x, alpha + y, beta + n - y), 
      from = 0, to = 1, 
      xlab = expression(pi), 
      ylab = "Density", 
      main = "Posterior distribution")

# Plot prior
curve(dbeta(x, alpha, beta), 
      from = 0, to = 1, 
      add = TRUE, 
      col = "red")

# Plot normalized likelihood
# We need to normalize the likelihood to make it integrate to 1
likelihood <- function(x) (1-x)^25
C <- integrate(likelihood, 0, 1)$value  # Normalizing constant
curve((1-x)^25/C, 
      from = 0, to = 1, 
      add = TRUE, 
      col = "blue")

legend("topright", 
       legend = c("Posterior", "Prior", "Likelihood"), 
       col = c("black", "red", "blue"), 
       lty = 1)
```

(ii) $\alpha = \beta = 1.0$ it is the uniform distribution in the interval $[0,1]$, is a prior that gives equal weight to the two possible outcomes. The posterior mean is $\frac{1.0}{1.0 + 1.0 + 25} = 0.038$.

```{r}
alpha <- 1.0
beta <- 1.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution")
curve(dbeta(x, alpha, beta), from = 0, to = 1, add = TRUE, col = "red")

likelihood <- function(x) (1-x)^25
C <- integrate(likelihood, 0, 1)$value
curve((1-x)^25 / C, from = 0, to = 1, add = TRUE, col = "blue")

legend("topright", legend = c("Posterior", "Prior", "Likelihood"), col = c("black", "red", "blue"), lty = 1)
```

(iii) $\alpha = \beta = 10.0$ is a prior that gives more weight to the value of $\pi = 0.5$. The posterior mean is $\frac{10.0}{10.0 + 10.0 + 25} = 0.222$.

```{r}
alpha <- 10.0
beta <- 10.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution")
curve(dbeta(x, alpha, beta), from = 0, to = 1, add = TRUE, col = "red")
likelihood <- function(x) (1-x)^25
curve((1-x)^25/C, from = 0, to = 1, add = TRUE, col = "blue")

legend("topright", legend = c("Posterior", "Prior", "Likelihood"), col = c("black", "red", "blue"), lty = 1)
```

```{r}
# plot of the three posterior distributions
alpha <- 0.5
beta <- 0.5
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, xlab = expression(pi), ylab = "Density", main = "Posterior distribution", col = "red")

alpha <- 1.0
beta <- 1.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, add = TRUE, col = "blue")

alpha <- 10.0
beta <- 10.0
n <- 25
y <- 0
curve(dbeta(x, alpha + y, beta + n - y), from = 0, to = 1, add = TRUE, col = "green")

legend("topright", legend = c("Jeffreys' prior", "Uniform prior", "Beta(10,10)"), col = c("red", "blue", "green"), lty = 1)
```

We can see that for more and more informative priors choices, the posterior distribution is shifting towards larger values of $\pi$, even for this case where there are no vegetarians in the sample.

It is worth noting that higher $\alpha$ and $\beta$ values will result in stronger prior belief (eg
Beta(10,10) prior is centered around 0.5 with high confidence). Indeed as we can see, the stronger prior is less influenced by the evidence from the data.

Moreover, we can see that the effects of the Jeffreys' prior and the one of the uniform distribution are pretty similar.


#### (b) 

If we were planning to take a larger survey from the same population, we could use the posterior results of the previous survey as the prior for the new one. For a new survey with $n'$ observations and $y'$ successes, the new posterior distribution, based on the previous posterior distribution $\text{Beta}(1,26)$, is given by:

$$
\text{Posterior}(\pi|y')=\text{Beta}(1 + y', 26 + n' - y')\,.
$$

## FSDS: Chapter 4, exercise 4.62

For the bootstrap method, explain the similarity and difference between the true sampling
distribution of $\hat{\theta}$ and the empirically-generated bootstrap distribution in terms of its center and its spread.

### Solution

In the frequentist (classical) approach, we assume that if we could repeat the sample many times, from the true distribution of the underlying random variable, we would be able to build the true sampling distribution of an estimator $\hat\theta$.

The idea of the bootstrap is to generate many bootstrap samples (i.e., samples with replacement from the original data) to approximate the sampling distribution of an estimator $\hat\theta$. In the *nonparametric bootstrap* approach, this involves computing the empirical distribution of the estimator from bootstrap replicates. Specifically, the set $\{\hat\theta_b^*\}_{b=1}^B$ provides these bootstrap replicates of the estimator $\hat\theta$.

The link with the frequentist approach is given by the fact that as the sample size approaches infinity, the empirical distribution of the random variable converges to its true underlying distribution.

##### Key differences:

- **Center:**

  - True sampling distribution centers on the true population parameter $\theta$
  $$
  \text{E}[\hat\theta]=\theta
  $$
  - Bootstrap distribution centers on the sample estimate $\hat\theta$, which approximates but may differ from the true value $\theta$.
  $$
  \text{E}[\hat\theta^*]=\hat\theta
  $$

- **Spread:**

  - True sampling distribution's spread reflects actual sampling variability in the population:
  $$
  \text{Var}(\hat\theta) = \text{E}[(\hat\theta-\theta)^2]\\ \theta=\text{E}[\hat\theta]
  $$
  where the expected value is take over the population distribution;
  
  - Bootstrap distribution's spread approximates this but tends to underestimate or overestimate the true variability since it resamples from a single finite sample rather than the full population.
  
  $$
  \widehat{\text{Var}}_{boot}=\frac{1}{B}\sum_{b=1}^B(\hat\theta_b^*-\hat\theta^*)^2
  $$
  $$
  \hat\theta^*=\frac{1}{B}\sum_{b=1}^B\hat\theta_b^*
  $$

## FSDS: Chapter 8, exercise 8.4

Refer to Exercise 8.1. Construct a classification tree, and prune strongly until the tree uses
a single explanatory variable. Which crabs were predicted to have satellites? How does the
proportion of correct predictions compare with the more complex tree in Figure 8.2?

### Solution

```{r, include=FALSE}
library(rpart)
library(rpart.plot)
library(caret)
```


```{r}
url <- "https://raw.github.com/stat4DS/data/main/Crabs.dat"
Crabs <- read.table(url, header = TRUE)
str(Crabs)
```


```{r}
# set y as a factor, required by rpart
Crabs$y <- as.factor(Crabs$y)
Crabs$color <- as.factor(Crabs$color)
Crabs$spine <- as.factor(Crabs$spine)

# Build the initial classification tree
fit <- rpart(y ~ weight + width + color + spine, data = Crabs)

# Plot a Complexity Parameter table for an Rpart Fit
plotcp(fit)

# Plot the tree
rpart.plot(fit)
```

##### Prune the Tree

Prune the tree to use a single explanatory variable by selecting cp such that nsplit = 1.

```{r}
# cp such that nsplit = 1
cp.onesplit<-fit$cptable[, "CP"][which(fit$cptable[, "nsplit"] ==1)]

# prune the tree
p.fit <- prune(fit, cp=cp.onesplit)
rpart.plot(p.fit, extra=1, digits=4, box.palette="auto")
```


From the tree shown above, we can conclude that crabs with width < 25.85 are predicted to have satellites.

###### Pruned Tree evaluation

Here, we calculate the proportion of correct predictions without cross-validation, (similarly to what was done in the book example).

```{r}
predict <- predict(p.fit, type = "prob" , newdata = Crabs)[,2]>0.5

# confusion matrix
confusion.m <- table(Crabs$y,predict)
confusion.m
```
```{r}
# accuracy
sum(diag(confusion.m))/nrow(Crabs)
```

We can see that the percentage of correct predictions is lower than the proportion of correct predictions in the example (0.751). However the pruned tree is considerably less complex.


## LAB: 
Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i\sim\text{Exponential}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

$π(λ)=\text{Beta}(4,2)$;

$π(λ)=\text{Normal}(1,2)$;

$π(λ)=\text{Gamma}(4,2)$;

Now, compute your posterior as $π(λ|y)\propto L(λ;y)π(λ)$ for the selected prior. If your first choice was correct, you will be able to compute it analytically.

### Solution

##### Prior Selection

Among the given options:

1. $π(λ)=\text{Beta}(4,2)$: Is not suitable, as it's defined on $[0,1]$.

2. $π(λ)=\text{Normal}(1,2)$: Is not suitable, as it's defined on $[-\infty, + \infty]$.

3. $π(λ)=\text{Gamma}(4,2)$: Is suitable, as it's defined on $\mathbb{R}^+$, considering that $\lambda > 0$.

##### Posterior Computation

- **Likelihood:** $y_i\sim\text{Exponential}(\lambda)$
- **Prior:** $π(λ)=\text{Gamma}(4,2)$

The posterior is derived as follows:

$$
\begin{aligned}
\text{Gamma}(4,2) &=\frac{2^4\lambda^3e^{-2\lambda}}{\Gamma(4)}=\frac{3}{8}\lambda^3e^{-2\lambda}\\
\pi(\lambda|y) &\propto\frac{3}{8}\lambda^3e^{-2\lambda}\prod_{i=1}^n\lambda e^{-\lambda y_i}\\
&\propto \lambda^{n+3}e^{-\lambda(2+\sum_i^n y_i)}
\end{aligned}
$$
Given the form of a Gamma distribution:

$$
f(\lambda; \alpha, \beta) = \frac{\beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda}}{\Gamma(\alpha)}, \quad \lambda \geq 0
$$


it follows that the posterior distribution is also a Gamma distribution with parameters $n+4$, $2+\sum_i^n y_i$

$$
\pi(\lambda|y) = \text{Gamma}(n+4, 2+\sum_i^n y_i)\, .
$$

Indeed as we can see the analytical solution is possible due to the conjugacy between the Exponential likelihood and Gamma prior.

## ISLR: chapter 6, exercises 6.9 (points (a)-(d))

In this exercise, we will predict the number of applications received using the other variables in the `College` data set.

(a) Split the data set into a training set and a test set.

(b) Fit a linear model using least squares on the training set, and report the test error obtained.

(c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

(d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

### Solution

#### (a): Split the Data
```{r}
College <- read.csv("https://www.statlearning.com/s/College.csv", header = TRUE)
# Remove the college name column
College <- College[, -1]


College$Private <- as.numeric(factor(College$Private))

# Set a seed for reproducibility
set.seed(1)

# Split the data into 75% training and 25% testing sets
n <- nrow(College)
idx.train <- sample(1:n, n*0.75, replace = FALSE)

College.train <- College[idx.train,]
College.test <- College[-idx.train,]
```

#### (b): Linear Regression
```{r}
# Get the response variable for the test set
y.test <- College.test$Apps

# Fit the linear regression model on the training set
linear.mod <- lm(Apps ~., data = College.train)

# Make predictions on the test set
predictions <- predict.lm(linear.mod, newdata=College.test)

errors <- y.test - predictions

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("Linear Regression RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("Linear Regression MAE:", round(mae, 2)))
```

#### (c): Ridge Regression
```{r}
library(glmnet)

# Prepare the data matrices for glmnet
X.train <- as.matrix(College.train[,-2])
X.test <- as.matrix(College.test[,-2])
y.train <- College.train$Apps
y.test <- College.test$Apps

# Perform cross-validation to choose optimal lambda
cvfit_ridge <- cv.glmnet(X.train,y.train, alpha = 0)

# Make predictions using the optimal lambda
ridge.pred <- as.numeric(predict(cvfit_ridge, newx = X.test, s = "lambda.min"))

errors <- y.test - ridge.pred

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("Ridge Regression RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("Ridge Regression MAE:", round(mae, 2)))
```

#### (d): Lasso Regression

```{r}
cvfit_lasso <- cv.glmnet(X.train,y.train, alpha = 1)

# Make predictions using the optimal lambda
lasso.pred <- as.numeric(predict(cvfit_lasso, newx = X.test, s = "lambda.min"))

errors <- y.test - lasso.pred

# Compute the Root Mean Squared Error (RMSE)
rmse <- sqrt(mean(errors^2))
print(paste("Lasso Regression RMSE:", round(rmse, 2)))

# Compute the Mean Absolute Error (MAE)
mae <- mean(abs(errors))
print(paste("Lasso Regression MAE:", round(mae, 2)))

# Number of non-zero coefficients in the Lasso model at the optimal lambda
non_zero_coef <- sum(coef(cvfit_lasso, s = "lambda.min") != 0) - 1  # Exclude the intercept
print(paste("Number of non-zero coefficients:", non_zero_coef))
```

```{r}
coef(cvfit_lasso, s = "lambda.min") 
```

Notably, the coefficient for `Terminal` has been shrunk to zero.

## ISLR: chapter 7, exercises 7.9

This question uses the variables `dis` (the weighted mean of distances to five Boston employment centers) and `nox` (nitrogen oxides concentration in parts per 10 million) from the `Boston` data. We will treat `dis` as the predictor and `nox` as the response. 

(a) Use the `poly()` function to fit a cubic polynomial regression to predict `nox` using `dis.` Report the regression output, and plot the resulting data and polynomial fits.

(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.

(d) Use the bs() function to fit a regression spline to predict `nox` using `dis.` Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.


### Solution

#### (a) Polynomial regression

```{r, echo=F}
# NOTE: I couldn't easily find the link for Boston data (if you want to search and swap it go ahead)
# Remove the previously downloaded dataset
rm(College) # To avoid warnings
# Get the data
library(ISLR2)
data(Boston)
```

```{r}
dis_range <- range(Boston$dis)
dis_grid <- seq(dis_range[1], dis_range[2], length.out = 100)

# Fit cubic polynomial regression
fit_poly <- lm(nox ~ poly(dis, 3), data = Boston)

# Generate predictions
preds <- predict(fit_poly, newdata = list(dis = dis_grid), se.fit = TRUE)

# Plot data and polynomial fit
par(mfrow = c(1, 1))
plot(Boston$dis, Boston$nox, xlab = "dis", ylab = "nox", col = "darkgrey", pch = 16, cex = 0.5, main = "Cubic Polynomial Fit")
lines(dis_grid, preds$fit, lwd = 2, col = "blue")

# Confidence intervals
se_bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
matlines(dis_grid, se_bands, lwd = 1, col = "blue", lty = 3)
```

#### (b) Plot polynomial fits for different polynomial degrees

```{r, fig.width=12, fig.height=8}
rss_values <- numeric(10)
dis_range <- range(Boston$dis)
dis_grid <- seq(dis_range[1], dis_range[2], length.out = 100)

# Set up the plot
par(mfrow = c(2, 5))

# Loop through polynomial degrees
for (degree in 1:10) {
  # Fit polynomial regression
  fit <- lm(nox ~ poly(dis, degree), data = Boston)
  
  # Calculate RSS
  rss_values[degree] <- sum(residuals(fit)^2)
  
  # Generate predictions
  preds <- predict(fit, newdata = list(dis = dis_grid))
  
  # Plot data and fit
  plot(Boston$dis, Boston$nox, xlab = "dis", ylab = "nox", col = "darkgrey", pch = 16, cex = 0.5,
       main = paste("Degree", degree))
  lines(dis_grid, preds, col = "blue", lwd = 2)
}

# Print the RSS values with degree names
for (degree in 1:10) {
  cat(paste("RSS for Degree", degree, "polynomial:", round(rss_values[degree], 4)), "\n")
}
```

```{r}
# Plot RSS as a function of degree
par(mfrow = c(1, 1))
plot(1:10, rss_values, type = "b", pch = 19, col = "red", xlab = "Polynomial Degree", 
     ylab = "RSS", main = "Residual Sum of Squares by Degree")
```

#### (c) Select the optimal degree for the polynomial

```{r, echo= F}
suppressMessages(library(boot))
```

```{r}
# Define k-fold CV function for polynomial degrees
cv_error <- numeric(10)
for (degree in 1:10) {
  fit <- glm(nox ~ poly(dis, degree), data = Boston)
  cv_error[degree] <- cv.glm(Boston, fit, K = 10)$delta[1]
}

# Plot CV error
plot(1:10, cv_error, type = "b", pch = 19, col = "red",
     xlab = "Polynomial Degree", ylab = "CV Error",
     main = "Cross-Validation Error by Polynomial Degree")
```
 
 The polynomial degree with the lowest cross-validation error is (degree 3 or 4); therefore, a sensible choice would be to use 3 degrees. This actually is not too surprising considering that higher-degree polynomials in general tend to overfit the data and oscillate quite a bit.

#### (d) Regression Splines 4 degrees of freedom

```{r}
library(splines)

# Fit the regression spline with 4 degrees of freedom
fit_spline <- lm(nox ~ bs(dis, df = 4), data = Boston)
summary(fit_spline)

dis_range <- range(Boston$dis)
dis_grid <- seq(dis_range[1], dis_range[2], length.out = 100)
preds <- predict(fit_spline, newdata = data.frame(dis = dis_grid), se.fit = TRUE)

# Plot the data and spline fit
par(mfrow = c(1, 1))
plot(Boston$dis, Boston$nox, col = "darkgrey", pch = 16, cex = 0.5, xlab = "dis", ylab = "nox", main = "Regression Spline (df = 4)")
lines(dis_grid, preds$fit, col = "blue", lwd = 2)
```

The knots are set uniformly...
TODO: Add description

#### (e) Regression Spline for a range of degrees of freedom


```{r, fig.width=12, fig.height=8}
# Set the range of degrees of freedom to test
df_range <- 3:10

rss_values <- numeric(length(df_range))
par(mfrow = c(2, 4))

# Fit splines for each degree of freedom and calculate the RSS
for (i in seq_along(df_range)) {
  df <- df_range[i]
  
  # Fit the regression spline model
  fit_spline <- lm(nox ~ bs(dis, df = df), data = Boston)
  rss_values[i] <- sum(residuals(fit_spline)^2)
  dis_grid <- seq(min(Boston$dis), max(Boston$dis), length.out = 100)
  preds <- predict(fit_spline, newdata = data.frame(dis = dis_grid))
  
  # Plot the data and the spline fit
  plot(Boston$dis, Boston$nox, col = "darkgrey", pch = 16, cex = 0.5, 
       xlab = "dis", ylab = "nox", main = paste("Spline Fit (df =", df, ")"))
  lines(dis_grid, preds, col = "blue", lwd = 2)
}

# Reset plot layout
par(mfrow = c(1, 1))

# Report RSS values
for (i in seq_along(df_range)) {
  cat(paste("RSS for Spline with", df_range[i], "df:", round(rss_values[i], 4)), "\n")
}
```

As we can see, when the degrees of freedom (df) increase, the variance also increases. The model follows the data more and more. This generally leads to a decrease in RSS but at the same time leads to overfitting, especially when the df are high.

#### (f) Perform cross-validation


## GAM

This question is about using gam for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The mcycle data in the MASS package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use gam to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use lm and poly to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by gam. Use termplot to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use gam to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

### Soltuion

---
title: "Motorcycle Crash Data Analysis"
output: html_document
---

```{r, echo=F}
suppressMessages(library(MASS))
suppressMessages(library(mgcv))
```

#### 1. GAM fit with GCV

```{r}
# Plot raw data and fit GAM
plot(mcycle$times, mcycle$accel, xlab="Time", ylab="Acceleration", 
     main="GAM Fit with GCV", pch=16)

# Fit GAM model with k=35
fit.1 <- gam(accel ~ s(times, k=35), data=mcycle)

lines(mcycle$times[order(mcycle$times)], 
      fitted(fit.1)[order(mcycle$times)], col="red", lwd=2)

points(mcycle$times, fitted(fit.1) + residuals(fit.1), 
       col="grey", pch="+")

# Legend for clarity
legend("topright", legend = c("Observed", "Smooth Fit", "Partial Residuals"),
       col = c("black", "red", "grey"), pch = c(16, NA, 3), lty = c(NA, 1, NA),
       lwd = c(NA, 2, NA))

summary(fit.1)
```

#### 2. Polynomial fit

```{r}
# Get effective degrees of freedom from GAM
edf <- round(summary(fit.1)$edf)

# Fit polynomial model
fit.2 <- lm(accel ~ poly(times, edf), data=mcycle)

# Plot polynomial fit with partial residuals
par(mfrow=c(1,1))
termplot(fit.2, partial.resid=TRUE, se=FALSE, main="Polynomial Fit")

# Legend for clarity
legend("topright", 
       legend = c("Polynomial Fit", "Partial Residuals"), 
       col = c("red", "grey"), 
       lty = c(1, NA), 
       pch = c(NA, 1),
       lwd = c(2, NA))

summary(fit.2)
```

#### 3. Unpenalized thin plate spline
```{r}

# Fit an Unpenalized tin plate regression spline using gam
fit.3 <- gam(accel ~ s(times, k = 35, bs = "tp"), data = mcycle, sp = 1e-10)

# Plot the un-penalized thin plate spline fit
plot(mcycle$times, mcycle$accel, xlab = "Time", ylab = "Acceleration",
     main = "Unpenalized tin plate spline", pch = 16, col = "black")
lines(mcycle$times[order(mcycle$times)], 
      fitted(fit.3)[order(mcycle$times)], col="red", lwd=2)

# Add partial residuals
points(mcycle$times, fitted(fit.3) + residuals(fit.3), col = "grey", pch = "+")

# Add legend for clarity
legend("topright", 
       legend = c("Observed", "Unpenalized TPS Fit", "Partial Residuals"), 
       col = c("black", "red", "grey"), 
       pch = c(16, NA, 3), 
       lty = c(NA, 1, NA), 
       lwd = c(NA, 2, NA))

summary(fit.3)
```

#### 4. Unpenalized cubic regression spline
```{r}

# Fit an Unpenalized tin plate regression spline using gam
fit.4 <- gam(accel ~ s(times, k = 35, bs = "cr"), data = mcycle, sp = 1e-10)

# Plot the un-penalized thin plate spline fit
plot(mcycle$times, mcycle$accel, xlab = "Time", ylab = "Acceleration",
     main = "Unpenalized cubic regression spline", pch = 16, col = "black")
lines(mcycle$times[order(mcycle$times)], 
      fitted(fit.4)[order(mcycle$times)], col="red", lwd=2)

# Add partial residuals
points(mcycle$times, fitted(fit.4) + residuals(fit.4), col = "grey", pch = "+")

# Add legend for clarity
legend("topright", 
       legend = c("Observed", "Unpenalized CRS Fit", "Partial Residuals"), 
       col = c("black", "red", "grey"), 
       pch = c(16, NA, 3), 
       lty = c(NA, 1, NA), 
       lwd = c(NA, 2, NA))

summary(fit.4)
```

Ordering of the models based on their acceptability of the results (GCV, Deviance Explained and Adjusted \( R^2 \)) would be in this following order: GAM with GC, TPS, CRS, Polynomial Regression.

Explanation:

##### *GAM with GC:*
- Has the lowest GCV of 565.15, indicating the best trade-off between fit and model complexity 
- Has the highest adjusted \( R^2 \) value of 0.78, which is also the best amongst all 4 models

##### *Unpenalized Thin Plate Spline:*
- Highest explained deviance (83%)
- Slightly lower Adjusted \( R^2 \) than GAM (0.771)
- Has a much higher GCV of 726.53, suggesting overfitting

##### *Unpenalized Cubic Regression Spline:*
- Even lower Adjusted \( R^2 \) of 0.766
- Devience explained is slightly lower thank the TPS model (82.6%)
- Also has a very high GCV of 742.52

##### *Polynomial Fit:*
- lowest Adjusted \( R^2 \) amongst all 4 models 0.7625 
- High RSE of 23.55, indicating a higher average deviation of residuals from the observed values



#### 5. Model residuals/time 
```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Residuals for GAM with GCV
plot(mcycle$times, residuals(fit.1), main = "Residuals: GAM with GCV", 
     xlab = "Time", ylab = "Residuals", pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Residuals for Polynomial Fit
plot(mcycle$times, residuals(fit.2), main = "Residuals: Polynomial Fit", 
     xlab = "Time", ylab = "Residuals", pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Residuals for Unpenalized Thin Plate Spline
plot(mcycle$times, residuals(fit.3), main = "Residuals: Thin Plate Spline", 
     xlab = "Time", ylab = "Residuals", pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# Residuals for Unpenalized Cubic Regression Spline
plot(mcycle$times, residuals(fit.4), main = "Residuals: Cubic Regression Spline", 
     xlab = "Time", ylab = "Residuals", pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)
```


##### *GAM with GC:*
- Residuals are evenly scattered around 0 across the time range
- No noticeable patterns, suggesting a well-fitted model without significant bias or overfitting

##### *Polynomial Fit:*
- Residuals show some oscillatory patterns, particularly at the edges of the time range
- Indicates overfitting or poor flexibility in capturing the underlying data structure

##### *Unpenalized Thin Plate Spline:*
- Residuals are generally close to 0 but show some clustering patterns at certain time intervals
- Suggests slight overfitting due to lack of penalization

##### *Unpenalized Cubic Regression Spline:*
- Similar to the thin plate spline, residuals are close to 0 but show some clustering
- Marginally less pronounced patterns compared to the thin plate spline, but still indicative of overfitting



#### 6. B-spline linear model
```{r}
knots <- quantile(mcycle$times, probs = seq(0, 1, length.out = 33))

# Fit B-spline model
fit.bs <- lm(accel ~ bs(times, degree = 3, knots = knots), data = mcycle)


# Plot the B-spline fit
plot(mcycle$times, mcycle$accel, xlab = "Time", ylab = "Acceleration", 
     main = "B-Spline Fit", pch = 16, col = "black")
lines(mcycle$times[order(mcycle$times)], 
      fitted(fit.bs)[order(mcycle$times)], col = "red", lwd = 2)

# Add residuals to the plot
points(mcycle$times, fitted(fit.bs) + residuals(fit.bs), col = "grey", pch = "+")

# Add legend
legend("topright", 
       legend = c("Observed", "B-Spline Fit", "Partial Residuals"), 
       col = c("black", "red", "grey"), 
       pch = c(16, NA, 3), 
       lty = c(NA, 1, NA), 
       lwd = c(NA, 2, NA))

summary(fit.bs)
```

##### *Comparison to other models:*
- Poorer fit quality compared to penalized spline approaches and GAM
- Has the same Adjusted \( R^2 \) as the CRS model (0.766)
- Slightly lower RSE than the polynomial model (23.37)
